<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Pendulum-Bot</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/scrolling-nav.css" rel="stylesheet">
    <link rel="icon" href="img/favicon.png">
</head>

<body id="page-top">

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" id="mainNav">
        <div class="container">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">Pendulum-Bot</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#intro">Introduction</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#design">Design</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#implementation">Implementation</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#results">Results</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#conclusion">Conclusion</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#team">Team</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#additional">Additional Materials</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <header class="bg-primary text-white">
        <div class="container text-center">
            <h1>Pendulum-Bot</h1>
            <p class="lead">A bot to catch a ball swinging from a pendulum.</p>
        </div>
    </header>

    <section id="intro">
        <div class="container">
            <div class="row">
                <div class="col-12 mx-auto">
                    <h2>Introduction</h2>
                    <p>
                        With robotics, the ability to understand and interact with the environment is of crucial importance. The general problem of studying trends in the environment and making predictions based on known information in order to act upon them is fascinating. This drove us to solve the problem of predicting a ball’s motion mid-air and catching it successfully. In our project, we train Baxter to reliably reach for and grab a ball while it is mid-air and in motion. This involves the use of computer vision and sensing to track the ball’s motion, contruction of a dynamics model of the system to predict where the ball is going, planning of the robot's motion to determine where the ball will be, and execution of the necessary movement to catch it. To complete this project we had to develop each of the individual components and also integrate them together in one cohesive system.
                    </p>
                    <p>
                        The ramifications of solving this are numerous – firstly, it is a demonstration of the capabilities of a robot’s interaction with a dynamic world (a more and more pressing topic as robots are gaining more mainstream popularity and usage, requiring them to work with a dynamic environment and model unknowns around them like cars and humans). The subproblems we tackle in computer vision and parameter estimation also have similar contemporary importance, letting this project a explore the capabilities available to robots when working with continuous dynamic systems.
                    </p>

                    <!--
                    <p>
                        The end goal of our system is as follows. Our system will use computer vision to track the movement of a tennis ball swinging like a pendulum. From here our code will generate a model of the system, which will enable us to track where the ball will be at a given time step in the future. Next we can move the robot to a optimal position from which we will be able to sense when the ball nears, and finally catch the ball from the air in the robot's gripper.
                    </p>
                    <p>
                        This problem combines many areas of robotics resulting in a interesting and diverse project. In order to initially track the ball we employed computer vision in order to track the ball in the view of the camera. To estimate where the ball is at any given time we had to derive the dynamics of the pendulum system and combine this with the input data in order to perform paramter estimation. After this we must select the optimal point at which the catch will be attempted, plan the path accordingly to avoid the path of the ball. Finally we must without much sensing of the ball's relative position to the end effector, we must effectively close the gripper on the ball to secure the catch. This project caught our attention because of the combination of all of these different concepts that we have learned in this class. To complete this project we had to develop each of the individual components and also integrate them together in one cohesive system.
                    </p>
                    <p>
                        There are a number of real world applications for this project as a whole as well as the individual components of this project. As a whole this project is related to the capture of a moving object which could be related to any task involving the tracking and capture of another object within the robot's workspace. This is relevant to the harder, but related problem of catching a thrown or falling ball. Extensions of this project include robots which catch falling objects, stop independently moving objects, or track and react to moving objects. The sub parts of this project have even more applications in the real world. The vision algorithms we implemented here are rather specific to the case of tennis balls, but these techniques could be applied to identify any number of different objects. Similarly the parameter estimation and pendulum modeling is related to a number of different real world problems in which a dynamic system is learned from observations. This is an immense number of applications which include things like tracking a falling ball, rotating systems, or other oscillatory motions.
                    </p>
-->

                </div>
            </div>
        </div>
    </section>

    <section id="design" class="bg-light">
        <div class="container">
            <div class="row">
                <div class="col-12 mx-auto">
                    <h2>Design</h2>
                    <p>
                        Given the goal of catching a ball mid-flight by the Baxter, a successful run will involve the Baxter being able to sense the trajectory of the ball, predict its future movement and grab the ball. Reliability is also a highly important metric of success.<br><br>

                        Of course, there were trade-offs we had to make from the original vision of Baxter simply catching a ball thrown towards it. Because of Baxter’s relatively slower arm motion speed, the problem was modified to make it more solvable given the existing resources. Instead of throwing the ball at Baxter (requiring the robot to model a parabolic motion influenced by gravity), we would use a pendulum setup instead. In this setup, with a pendulum hanging in front of the robot, the objective of Baxter is precisely to catch the ball while mid-oscillation reliably by modeling future movement. <br><br>

                        The complexity of the problem is still preserved, as the system still calls for modelling the oscillatory motion of the pendulum bob. However, the physical constraints of the robot are now respected, making the task at hand feasible. Furthermore, achieving reliability is a lot easier to ensure because of the presence of a more constrained pendulum rig instead of a freeform ball toss.<br><br>

                        In the completion of this project we had the following goals which we wished to acheive in order to finish the project:
                    </p>
                    <ul>
                        <li>Build a pendulum rig for the ball to swing from.</li>
                        <li>Build a gripper attachment to increase catchability.</li>
                        <li>Implement computer vision algorithm on a static image to identify the location of the ball’s center.</li>
                        <li>Implement computer vision on live video feed to identify the location of the ball’s center as the ball moves and the calculate the distance away the ball is from the camera frame.</li>
                        <li>Movement of the robot end effecor to the position of a static ball.</li>
                        <li>Movement of the robot end effector to the position of a swinging ball.</li>
                        <li>Catching/stopping the ball with the robot end effector.</li>
                        <li>Integreation of all of the above components into a system which can detect the motion of the ball, predict where the ball will be, move to a optimal catching position and make the catch.</li>
                    </ul>
                    <p>
                        Already we have roughly discussed the overall design of our robot, but here we will clarify the large components of our design and how they fit together.
                    </p>
                    <p class="lead">Infrastructure</p>
                    <p>
                        In order to complete the project we need to build a rig for our pendulum and an enhanced gripper. The rig for the pendulum need not be complex, for this we only need something that will allow a ball to swing back and forth reliably without losing too much energy. For the gripper, we decided that the margin of error with the standard gripper would be too high. In order to mitigate any issues with this we will create a mitt like attachment for the gripper which will dramatically improve our chances of being able to execute a successful catch.
                    </p>
                    <p class="lead">Vision</p>
                    <p>
                        The first computational step of this process is the identification of where the ball will be with respect to the base frame of the robot. In order to determine this we will be using computer vision. Using computer vision libraries we will be able to determine where the ball is centered in the frame of the camera. From here we will be using a kinect sensor, which will allow us to additionally determine the distance away of the ball from the camera. From these measurements we will be able to publish the coordinates of the ball location with respect to the base frame. To correctly determine these coordinates we will also have to compute the transform from the coordinates in the kinect frame into the coordinates with respect to the base frame, which will be done using a static transform.
                    </p>
                    <p class="lead">Modeling</p>
                    <p>
                        Given the topic of the ball location which is being published, we can feed these datapoints into our model for the pendulum. This software will take in the data and from there extrapolate the parameters of the system. Once these parameters have been estimated, we will then have a rough prediction of the balls location as time progresses. This will be vital for later parts of the project. As mentioned above the robot is rather slow, so being able to predict where the robot will be in the future is needed in order to successfully catch the ball.
                    </p>
                    <p class="lead">Planning and Execution</p>
                    <p>
                        Once our model has accurately determined the trajectory of the ball, we can move onto the final stage of the process--the catch itself. Our software determines at which location along the ball's predicted trajectory will be the most optimal location for the ball to be caught. Once this has been determined, we will employ an inverse kinematics solver which will compute a path for the arm to move the end effecor into the desired position. For this we will have several obstacles/boundaries to ensure that the robot's movement does not interrupt the ball. Once the ball has moved into the desired location we will switch sensing back to using our active vision tracking, which will allow us to have feedback from the environment. If the ball comes within a certain distance from the end effector, we will begin to close the gripper. If all goes according to plan the timing is perfect and the gripper closes just as the ball comes between it.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <section id="implementation">
        <div class="container">
            <div class="row">
                <div class="col-12 mx-auto">
                    <h2>Implementation</h2>
                    <p>
                        The design has been described above. Now we will discuss how we actually went about implementing each of these sections. Additionally we include a description of how the software for this program was written and how each of the individual components/ROS nodes actually fit togehter.
                    </p>
<!--
                    <pre>
                        while (true):
                            print("test")
                    </pre>
-->
                    <p class="lead">Infrastructure</p>
                    <p>
                        <p>
                            For our pendulum, we considered several mor complicated designs for the creation of a pendulum. We ultimately found that using a mic stand worked well enough for what we needed for this project. If we were to extend this project a good next step would be to build a more precise rig that would allow for smoother more reliable motion. Our makeshift pendulum stand is pictured below.
                        </p>
                        <img src="img/pendulum-rig.jpg" alt="Pendulum Rig" class="img-fluid auto-mx">
                        <p>
                            Our gripper is rather primitive, but is a proof of concept for what could be used in the future if we were to continue working on this project. For the intents and purposes of our project this was sufficient for us to use to help us achieve our other goals. In the future it would be nice to have a more acutated gripper which would allow for the gripper to be opened wider and closed tigher. The gripper attachement is needed as can be seen below. Without the attachment the ball can barely be grasped by the gripper and catching the ball would have been nearly impossible. Pictured below is the gripper before and after the addition of our attachment. The overall design just involves two larger plates which are attached directly to the original gripper attachments.
                        </p>
                        <img src="img/grip.jpg" alt="Pendulum Rig" class="img-fluid auto-mx">
                        <img src="img/ball-grip.jpg" alt="Pendulum Rig" class="img-fluid auto-mx">
                        <img src="img/gripper.jpg" alt="Pendulum Rig" class="img-fluid auto-mx">
                        <p class="lead">Vision</p>
                        <p>
                            The main goal of this part of the project is to be able to determine the location of the ball with respect to the kinect camera frame. The first part of this involved the calculation of where the ball is in the x-y plane of the image. Before dealing with a live image, we first decided to solve the simpler problem of identifying the location of a ball in a static image. After we had master this simpler problem we proceded onto the main algorithm which will compute the coordinates of the ball in the kinect frame solely from the data generated by the kinect. The pseudoocode of our complete vision processing algorithm is as follows.
                        </p>
                        <ul>
                            <li>On the first image that is received we allow for the user to select four points which will be used to define the area of the image in which the vision algorithm should look for the ball. This is done once on initialization and assumed to be correct for the remainder of the execution of the program.</li>
                            <li>Once the bounding box has been calibrated we will do the following on each image frame received.</li>
                            <li>Convert the image data from the image message into a cv image so that it can be modified using OpenCV.</li>
                            <li>Next we blur the image using a gaussian blur. This step is done to reduce the effects of noise.</li>
                            <li>We then create a mask of the image through color thresholding. If pixels are within the correct range of colors (as was calculated experimentally) we set the value of that pixel to be true, otherwise the pixels are false. This should generate an image that shows the shapes of objects that are the color of a tennis ball.</li>
                            <li>Using OpenCV, we then find a list of all of the contours in the masked image.</li>
                            <li>After we have the list of all of the contours calculated, we then find the largest contour (as determined by the contour area). This is assumed to be a contour which corresponds to the ball in the frame.</li>
                            <li>Next from the largest contour we find the moments of the contour. If the contour's radius meets the criteria of the minimum radius, we assume that this is indeed the location of the ball in the 2D image. This already has encoded the center point of the contour, which we will assume from this point on to be the current center of the ball.</li>
                            <li>The next step will be the calculation of the depth of the ball from the kinect. This is handled through the <code>getDepth</code> function whose behavior is described in the following steps.</li>
                            <li>Because of issues we encountered with calculating the location of the ball in the frame as well as the fact that there is not a one to one correspondence of the pixels of the RGB kinect camera and the kinect's depth sensor, we had to look for an approximate solution of the (x, y, z) location of the ball with respect to the kinect's frame.</li>
                            <li>We iterate over 500 samples of a normal distribution of points around the calculated ball's 2D position. We then find the minumum distance away from the kinect frame using the point cloud. We assume that the minimum distance is the true distance, as there will be nothing obstructing the ball. Before doing this the algorithm would often mis-identify the depth as something much larger, using the depth values from behind the ball. </li>
                            <li>From here we have calculated the (x, y, d) position of the ball with respect to the kinect frame which will then be returned.</li>
                        </ul>
                        <p class="lead">Modeling</p>
                        <p>
                            Now that vision lets us find the position of the ball at any moment, we can model the trajectory of the pendulum. What we did in order to achieve this was as follows. 
                        </p>
                        <p>
                            We use the three-dimensional equations to model our pendulum oscillatory motion below. (For further details on the derivation, refer to the additional information). Below is an example modelled trajectory of a path given parameters. The blue dot is the pivot, and the grey line the possible trajectory it could take given a starting position marked by the translucent dot.
                        </p>
                        <img src="img/trajectory.png" alt="trajectory" class="img-fluid auto-mx">
                        <p>
                            It is important to note that our equation makes some simplifying assumptions. As we observe the dynamics of phi and theta (where phi and theta are in the diagram marked below), we notice how theta is the only changing angle. Because of our model, we only assume that theta is the changing parameter. phi remains constant. We do not account for damping in the value of theta because our robot aims to catch the pendulum before a full oscillation, making the effect of air resistance negligible, especially given our closeness threshold feedback system that helps minimize errors caused in the planning.
                        </p>
                        <img src="img/p_angles.png" alt="theta and phi" class="img-fluid auto-mx">
                        <img src="img/thetavphi.png" alt="graph of theta v phi" class="img-fluid auto-mx">
                        <p class="lead">Planning and Execution</p>
                        <p>
                            With the aforementioned modelling in play, all that is needed left is executing the plan of motion. An important aspect of this planning is the orientation of the gripper, since we need to be able to catch the ball. To get the correct correct orientation of the gripper, we needed to get the frame of the right_hand with respect to the body frame. To do this, because we knew  we wanted the gripper to be facing down, we knew the z axis (with respect to the gripper) would be facing the floor, the x axis would be along the projection the pendulum follows along the base's xy's plane, and the y axis would be orthogonal to that projection. We calculated this rotation matrix and converted it to a quaternion which would give the orientation for the pose.
                        </p>
                        <img src="img/rotation_calc.jpg" alt="rotation calculation" class="img-fluid auto-mx">
                        <p class="lead">Software Description</p>
                        <p>
                            The overall structure of the project can be described in the below flow chart. Below we describe in more detail the launch files we used, the topics we are publishing/subscribing to, and the ROS nodes which are present.
                        </p>
                        <img src="img/pendulum-bot-flow.svg" alt="flowchart" class="img-fluid auto-mx">
                        <h5></h5>
                        <h5>Topics</h5>
                        <ul>
                            <li><code>camera/rgb/image_raw</code> - This topic is created by the kinect and represents the RGB readouts from the kinect camera. This is the image that we will run computer vision on.</li>
                            <li><code>camera/rgb/camera_info</code> - This topic is also created by the kinect and represents some camera metadata including the size of the image we are looking at.</li>
                            <li><code>camera/depth/points</code> - This is the last topic created by the kinect that we are using. This represents the depth pointcloud created by the kinect which we ultimately use to calculate the third coordinate in the kinect frame.</li>
                            <li><code>kinect/ball/location</code> - This is a topic created by our code which represents that current location of the ball in coordinates with respect to the kinect frame.</li>
                            <li><code>base/ball/location</code> - This is a topic created by our code which represents that current location of the ball in coordinates with respect to the base frame of the robot.</li>
                            <li><code>ball/goal</code> - This is a topic created by our code which represents the optimal location we have found at which the catch should be performed. Before the model finishes, this topic publishes no information.</li>
                            <li><code>ball/ready</code> - This is a topic created by our code which publishes only a boolean value representing if the robot has moved to the pose specified in the <code>ball/goal</code> topic. If true that means the movement has finished and the gripper can close once the ball nears the end effector.</li>
                            <li><code>robot/xdisplay</code> - This is a topic that we publish to so that we can see the kinect camera image on the screen of the robot.</li>
                        </ul>
                        <h5>ROS Nodes</h5>
                        <ul>
                            <li><code>kinect_util</code> - This is our own program. The high level function of this node is to parse the information output by the kinect and turn this into the information published to the <code>kinect/ball/location</code> topic. The complete process for how this program works is described above in more detail in the section on vision.</li>
                            <li><code>to_world_frame</code> - This is our own program which subscribes to <code>kinect/ball/location</code> and outputs the transformed coordinates in the base frame as the topic <code>base/ball/location</code>.</li>
                            <li><code>model</code> - This too is a program written by us which handles the parameter estimation and modelling of the pendulum. The specific details of how this program works are described above in the model section. At a high level this node subscribes to <code>base/ball/location</code> and then published the optimal catch position to the topic <code>ball/goal</code></li>
                            <li><code>move_to_goal</code> - This is our program which handles the path planning and execution of the path. This node subscribes to <code>ball/goal</code> in order to see where the end effector should move. It then publishes to <code>ball/ready</code> when the movement has been completed and the end effector is in the desired position.</li>
                            <li><code>gripper</code> - This is the node that we wrote to control the gripper. In order to do this this node subscribes to <code>base/ball/location</code> and <code>ball/ready</code> to determine when the gripper is ready to be closed and when the ball is near enough to be closed such that the catch can be made. Once both of these are satisfied, the gripper will close with the ball in its grasp.</li>
                            <li><code>robot_state_publisher</code> - This is a node which is used for the visualization fo the robot in RViz and is also used to determine the joint state of the robot. This relies on the URDF file of the robot (baxter_description /urdf/baxter.urdf).</li>
                            <li><code>joint_state_publisher</code> - This is a node very similar to <code>robot_state_publisher</code> which is used for the visualization of the robot in RViz and is also used to determine the joint state of the robot. This relies on the URDF file of the robot (baxter_description /urdf/baxter.urdf).</li>
                            <li><code>trajectory_server</code> - This is a part of the baxter interface which is used to actually perform the actuation of the robot.</li>
                            <li><code>display_to_kinect_transformer</code> - This is a node which publishes the static transform form head_camera to camera_link (this is the frame of the kinect).</li>
                        </ul>
                        <h5>Launch Files</h5>
                        <ul>
                            <li><code>baxter_imaging pendulum.launch</code> - This is the primary launch file that we use to bring up all all of the nodes listed above. This additionally causes the other two below launch files to be run.</li>
                            <li><code>freenect_launch freenect.launch</code> - This launch file will bring up the kinect and the topics related to the kinect </li>
                            <li><code>baxter_moveit_config demo_baxter.launch</code> - This launch file allows us to give MoveIt commands as well as see and control Baxter in RVIZ</li>
                        </ul>
                </div>
            </div>
        </div>
    </section>

    <section id="results" class="bg-light">
        <div class="container">
            <div class="row">
                <div class="col-12 mx-auto">
                    <h2>Results</h2> 
                    <p>
                        The following 3 photso are the visualization of Baxter in it's current state with an overlay RGB-D image of what the connect sees. This shows that the static transform that connects the kinect to baxter is accurate as you can see the table, the pendulum and the ball. Additoinally, the pink sphere in the picture is the location of the PointStamped message that represents the ball's location in the real-world. This additionally shows that we were able to accurately calculate where the ball is hanging in free space.
                    </p><img src="img/baxter_and_ball.png" alt="baxter and ball" class="img-fluid auto-mx">
                    <img src="img/baxter_and_ball_2.png" alt="baxter and ball 2" class="img-fluid auto-mx">
                    <img src="img/baxter_far_away.png" alt="baxter and surroundings" class="img-fluid auto-mx">
                    <p>
                        Although we were unhappy with the results from the demo, we are extremely please with the outcome of the total project.
                        We were able to reach our goals despite many obstacles and are thoroughly proud of what we have accomplished. We were able to
                        successfully track the ball in free space, put it in scope and perspective of baxter, calculate a model prediction of where
                        the ball will end up, move Baxter to a position and orientation that would allow it to catch the ball, and finally catch the ball.
                        Please enjoy this video of a full explanation and demo of the entire project:
                    </p>
                    <div class="embed-responsive embed-responsive-16by9">
                        <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/SnMkngNq84M?rel=0" allowfullscreen></iframe>
                    </div>
                    <p>
                    Here's a gif of it catching from a completely different setup (<a href="https://www.youtube.com/u0ePHx_Uj48">Click Here to watch video</a>):
                    </p>
                    <img src="img/catch.gif" class="img-fluid auto-mx">
                    <p>
                    And another: (<a href="https://www.youtube.com/bTfPsS0wAnY">Click Here to watch video</a>):
                    </p>
                    <img src="img/catch1.gif" class="img-fluid auto-mx">
                    <p>
                    And yet another, however, it didn't close the gripper in time (<a href="https://www.youtube.com/l7rDBGkgkKM">Click Here to watch video</a>):
                    </p>
                    <img src="img/catchfail.gif" class="img-fluid auto-mx">
                    <p>
                        We are very proud of the fact that we built a model good enough to catch a swinging ball. To highlight this fact, here is an image of the
                        margin of error we were dealing with (i.e. the space that the ball could actually fit through so the gripper could actually hold onto the 
                        ball):
                    </p>
                    <img src="img/margin_of_error.jpg" class="img-fluid auto-mx">
                </div>
            </div>
            <div class="row">
                <div class="col-12 mx-auto">
                    <p>
                        This is an additional video that demonstrates our capability to track the ball's motion.
                    </p>
                    <div class="embed-responsive embed-responsive-16by9">
                        <video controls="controls" name="Vision Demo" src="img/vision_demo.mov"></video>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section id="conclusion">
        <div class="container">
            <div class="row">
                <div class="col-12 mx-auto">
                    <h2>Conclusion</h2>
                    <p>
                        Overall, through this project, we were able to accomplish the goal we strove to reach! Our arm was able to sufficiently 
                        model the pendulum motion, ready itself and catch the ball mid-flight. There is definitely room for expansion 
                        and growth, in perhaps doing parameter estimation by letting the pendulum oscillate enough for the robot to infer.
                        We encountered many difficulties along the way:
                        <ul>
                            <li>Poor quality cameras on Baxter</li>
                            <li>Depth calculation using Kinect PointCloud from RGB (come from two sources resulting in a non-one-to-one pixel correlation)</li>
                            <div class="row">
                                <div class="col-4 mx-auto">
                                    <img src="img/depth.jpg" alt="depth output" class="img-fluid auto-mx">
                                </div>
                                <div class="col-4 mx-auto">
                                    <img src="img/rgb.jpg" alt="rgb output" class="img-fluid auto-mx">
                                </div>
                            </div>
                            <i>If you look at the bottom left corner of where the table should be, you can clearly see that the rgb image is shifted to the right by some number of pixels</i>
                            <li>Ball recognition and transformation to real world coordinates</li>
                            <li>Time synchronization between ROS Master and reacting in real-time</li>
                        </ul>
                        We were able to successfully overcome the first three difficulties, but because of the last difficulty, we did have to do a manual
                        gripper control because we just couldn't sync up the time well enough. Overall, we are very satisfied with the progress that we have
                        made and look forward to exploring the bounds of this project in the future!
                    </p>
                </div>
            </div>
        </div>
    </section>

    <section id="team" class="bg-light">
        <div class="container">
            <div class="row my-5">
                <div class="col-12 mx-auto">
                    <h2>Team</h2>
                    <div class="row">
                        <div class="col-4  mx-auto">
                            <img src="img/cam.jpg" alt="Cameron" class="img-fluid auto-mx">
                        </div>
                        <div class="col-8 mx-auto">
                            <p class="lead"> <b>Cameron Kurotori</b></p>
                            <p>
                                Cameron is an EECS student with a background in algorithms, task automation, and software engineering. He has taken/is taking CS170, CS70, CS61 series as well as the EE16 series. This past summer he had the opportunity to be a software developer intern at IBM.
                            </p>
                            <p>
                                The main contributions of Cameron were towards the construction of the pendulum rig, the vision processing code, and the code to compute the transformation from the camera frame of the kinect to the frame of the robot. He additionally worked on the overall system design and helped to write much of the code to create the subscribers, publishers, and other ROS nodes.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
            <div class="row my-5">
                <div class="col-12 mx-auto">
                    <div class="row">
                        <div class="col-4 mx-auto">
                            <img src="img/matt.png" alt="Matt" class="img-fluid auto-mx">
                        </div>
                        <div class="col-8 mx-auto">
                            <p class="lead"> <b>Matt Owen</b></p>
                            <p>
                                Matt is an EECS student with a background in algorithms, robotics and aritifical intelligence. I have taken/am taking EE126, CS170, CS188, and DS100. In high school he was involved in robotics for four years primarily from the mechanical persepctive. Additionally he has taken classes in CAD and computer integrated manufacturing.
                            </p>
                            <p>
                                The main contributions of Matt were towards the construction of the gripper attachment, the vision processing code, the code to compute the transformation from the camera frame of the kinect to the frame of the robot, and the code to close the gripper. He also provided general assistance to the team for the other parts of the project and wrote the report/created the website along with the help of the others.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
            <div class="row my-5">
                <div class="col-12 mx-auto">
                    <div class="row">
                        <div class="col-4  mx-auto">
                            <img src="img/sean.jpg" alt="Sean" class="img-fluid auto-mx">
                        </div>
                        <div class="col-8 mx-auto">
                            <p class="lead"> <b>Sean Farhat</b></p>
                            <p>
                                Sean is an EECS student with a background in algorithms, artificial intelligence, and low-level programming, having taken courses in both the EE, CS, Math, and Cognitive Science departments. He also does research in reinforcement learning and controls for microrobots. Additionally he has experience with computer vision and OpenCV.
                            </p>
                            <p>
                                The main contributions of Sean were in the dynamics modeling of the pendulum system, the parameter estimation, the code to plan/move the arm, and the code to close the gripper. He also provided general assistance to the team for the other parts of the project.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
            <div class="row my-5">
                <div class="col-12 mx-auto">
                    <div class="row">
                        <div class="col-4  mx-auto">
                            <img src="img/swapnil.jpg" alt="Swapnil" class="img-fluid auto-mx">
                        </div>
                        <div class="col-8 mx-auto">
                            <p class="lead"> <b>Swapnil Das</b></p>
                            <p>
                                Swapnil is an EECS student with a background in algorithms, artificial intelligence and controls. He has taken CS170, CS188, EE120 and is taking EECS149 currently. His past research experience delves deeply into low-level optimization, parallelization and regression algorithms. He is currently also learning low-level control of Kobuki in EECS149 and state machine generation.
                            </p>
                            <p>
                                The main contributions of Swapnil were in the dynamics modeling of the pendulum system, the parameter estimation, the code to plan/move the arm, and the code to close the gripper. He also provided general assistance to the team for the other parts of the project.
                            </p>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section id="additional">
        <div class="container">
            <div class="row">
                <div class="col-12 mx-auto">
                    <h2>Additional Materials</h2>
                    <p class="lead">Our code</p>
                    <p>
                        The full code for this system can be found <a href="https://github.com/omatthew98/pendulum-bot">here</a>.
                    </p>
                    <p class="lead">Resources</p>
                    <ul>
                        <li>
                            The computer vision code we used was a modification of the code by Adrian Rosebrock from pyimagesearch. The tutorial we followed is <a href="https://www.pyimagesearch.com/2015/09/14/ball-tracking-with-opencv/">here</a>.
                        </li>
                    </ul>
                    <p>
                        Below are the derivations made to prove the 3-dimensional model.
                    </p>
                    <img src="img/derivations.png" alt="derivations" class="img-fluid auto-mx">
                </div>
            </div>
        </div>
    </section>



    <!-- Footer -->
    <footer class="py-5 bg-dark">
        <div class="container">
            <p class="m-0 text-center text-white">Copyright &copy; Pendulum-Bot 2018</p>
        </div>
        <!-- /.container -->
    </footer>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Custom JavaScript for this theme -->
    <script src="js/scrolling-nav.js"></script>

</body>

</html>
