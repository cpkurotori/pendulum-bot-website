<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Pendulum-Bot</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/scrolling-nav.css" rel="stylesheet">
    <link rel="icon" href="img/favicon.png">
</head>

<body id="page-top">

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" id="mainNav">
        <div class="container">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">Pendulum-Bot</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#intro">Introduction</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#design">Design</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#implementation">Implementation</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#results">Results</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#conclusion">Conclusion</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#team">Team</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link js-scroll-trigger" href="#additional">Additional Materials</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <header class="bg-primary text-white">
        <div class="container text-center">
            <h1>Pendulum-Bot</h1>
            <p class="lead">A bot to catch a ball swinging from a pendulum.</p>
        </div>
    </header>

    <section id="intro">
        <div class="container">
            <div class="row">
                <div class="col-8 mx-auto">
                    <h2>Introduction</h2>
                    <p>
                        With robotics, the ability to understand and interact with the environment is of crucial importance. The general problem of studying trends in the environment and making predictions based on known information in order to act upon them is fascinating. This drove us to solve the problem of predicting a ball’s motion mid-air and catching it successfully. In our project, we train Baxter to reliably reach for and grab a ball while it is mid-air and in motion. This involves the use of computer vision and sensing to track the ball’s motion, contruction of a dynamics model of the system to predict where the ball is going, planning of the robot's motion to determine where the ball will be, and execution of the necessary movement to catch it. To complete this project we had to develop each of the individual components and also integrate them together in one cohesive system.
                    </p>
                    <p>
                        The ramifications of solving this are numerous – firstly, it is a demonstration of the capabilities of a robot’s interaction with a dynamic world (a more and more pressing topic as robots are gaining more mainstream popularity and usage, requiring them to work with a dynamic environment and model unknowns around them like cars and humans). The subproblems we tackle in computer vision and parameter estimation also have similar contemporary importance, letting this project a explore the capabilities available to robots when working with continuous dynamic systems.
                    </p>

                    <!--
                    <p>
                        The end goal of our system is as follows. Our system will use computer vision to track the movement of a tennis ball swinging like a pendulum. From here our code will generate a model of the system, which will enable us to track where the ball will be at a given time step in the future. Next we can move the robot to a optimal position from which we will be able to sense when the ball nears, and finally catch the ball from the air in the robot's gripper.
                    </p>
                    <p>
                        This problem combines many areas of robotics resulting in a interesting and diverse project. In order to initially track the ball we employed computer vision in order to track the ball in the view of the camera. To estimate where the ball is at any given time we had to derive the dynamics of the pendulum system and combine this with the input data in order to perform paramter estimation. After this we must select the optimal point at which the catch will be attempted, plan the path accordingly to avoid the path of the ball. Finally we must without much sensing of the ball's relative position to the end effector, we must effectively close the gripper on the ball to secure the catch. This project caught our attention because of the combination of all of these different concepts that we have learned in this class. To complete this project we had to develop each of the individual components and also integrate them together in one cohesive system.
                    </p>
                    <p>
                        There are a number of real world applications for this project as a whole as well as the individual components of this project. As a whole this project is related to the capture of a moving object which could be related to any task involving the tracking and capture of another object within the robot's workspace. This is relevant to the harder, but related problem of catching a thrown or falling ball. Extensions of this project include robots which catch falling objects, stop independently moving objects, or track and react to moving objects. The sub parts of this project have even more applications in the real world. The vision algorithms we implemented here are rather specific to the case of tennis balls, but these techniques could be applied to identify any number of different objects. Similarly the parameter estimation and pendulum modeling is related to a number of different real world problems in which a dynamic system is learned from observations. This is an immense number of applications which include things like tracking a falling ball, rotating systems, or other oscillatory motions.
                    </p>
-->

                </div>
            </div>
        </div>
    </section>

    <section id="design" class="bg-light">
        <div class="container">
            <div class="row">
                <div class="col-8 mx-auto">
                    <h2>Design</h2>
                    <p>
                        Given the goal of catching a ball mid-flight by the Baxter, a successful run will involve the Baxter being able to sense the trajectory of the ball, predict its future movement and grab the ball. Reliability is also a highly important metric of success.<br><br>

                        Of course, there were trade-offs we had to make from the original vision of Baxter simply catching a ball thrown towards it. Because of Baxter’s relatively slower arm motion speed, the problem was modified to make it more solvable given the existing resources. Instead of throwing the ball at Baxter (requiring the robot to model a parabolic motion influenced by gravity), we would use a pendulum setup instead. In this setup, with a pendulum hanging in front of the robot, the objective of Baxter is precisely to catch the ball while mid-oscillation reliably by modeling future movement. <br><br>

                        The complexity of the problem is still preserved, as the system still calls for modelling the oscillatory motion of the pendulum bob. However, the physical constraints of the robot are now respected, making the task at hand feasible. Furthermore, achieving reliability is a lot easier to ensure because of the presence of a more constrained pendulum rig instead of a freeform ball toss.<br><br>

                        In the completion of this project we had the following goals which we wished to acheive in order to finish the project:
                    </p>
                    <ul>
                        <li>Build a pendulum rig for the ball to swing from.</li>
                        <li>Build a gripper attachment to increase catchability.</li>
                        <li>Implement computer vision algorithm on a static image to identify the location of the ball’s center.</li>
                        <li>Implement computer vision on live video feed to identify the location of the ball’s center as the ball moves and the calculate the distance away the ball is from the camera frame.</li>
                        <li>Movement of the robot end effecor to the position of a static ball.</li>
                        <li>Movement of the robot end effector to the position of a swinging ball.</li>
                        <li>Catching/stopping the ball with the robot end effector.</li>
                        <li>Integreation of all of the above components into a system which can detect the motion of the ball, predict where the ball will be, move to a optimal catching position and make the catch.</li>
                    </ul>
                    <p>
                        Already we have roughly discussed the overall design of our robot, but here we will clarify the large components of our design and how they fit together.
                    </p> 
                    <p class="lead">Infrastructure</p>
                    <p>
                        In order to complete the project we need to build a rig for our pendulum and an enhanced gripper. The rig for the pendulum need not be complex, for this we only need something that will allow a ball to swing back and forth reliably without losing too much energy. For the gripper, we decided that the margin of error with the standard gripper would be too high. In order to mitigate any issues with this we will create a mitt like attachment for the gripper which will dramatically improve our chances of being able to execute a successful catch.
                    </p>
                    <p class="lead">Vision</p>
                    <p>
                        The first computational step of this process is the identification of where the ball will be with respect to the base frame of the robot. In order to determine this we will be using computer vision. Using computer vision libraries we will be able to determine where the ball is centered in the frame of the camera. From here we will be using a kinect sensor, which will allow us to additionally determine the distance away of the ball from the camera. From these measurements we will be able to publish the coordinates of the ball location with respect to the base frame. To correctly determine these coordinates we will also have to compute the transform from the coordinates in the kinect frame into the coordinates with respect to the base frame, which will be done using a static transform.
                    </p>
                    <p class="lead">Modeling</p>
                    <p>
                        Given the topic of the ball location which is being published, we can feed these datapoints into our model for the pendulum. This software will take in the data and from there extrapolate the parameters of the system. Once these parameters have been estimated, we will then have a rough prediction of the balls location as time progresses. This will be vital for later parts of the project. As mentioned above the robot is rather slow, so being able to predict where the robot will be in the future is needed in order to successfully catch the ball. 
                    </p>
                    <p class="lead">Planning and Execution</p>
                    <p>
                        Once our model has accurately determined the trajectory of the ball, we can move onto the final stage of the process--the catch itself. Our software determines at which location along the ball's predicted trajectory will be the most optimal location for the ball to be caught. Once this has been determined, we will employ an inverse kinematics solver which will compute a path for the arm to move the end effecor into the desired position. For this we will have several obstacles/boundaries to ensure that the robot's movement does not interrupt the ball. Once the ball has moved into the desired location we will switch sensing back to using our active vision tracking, which will allow us to have feedback from the environment. If the ball comes within a certain distance from the end effector, we will begin to close the gripper. If all goes according to plan the timing is perfect and the gripper closes just as the ball comes between it. 
                    </p>
                    <p class="lead">Talk about the design of the robot here. What design criteria must your project meet? What is the desired
                        functionality? Describe the design you chose. What design choices did you make when you formulated your design? What
                        trade-offs did you have to make? How do these design choices impact how well the project meets design criteria that
                        would be encountered in a real engineering application, such as robustness, durability, and efficiency?
                    </p>
                </div>
            </div>
        </div>
    </section>

    <section id="implementation">
        <div class="container">
            <div class="row">
                <div class="col-8 mx-auto">
                    <h2>Implementation</h2>
                    <p>
                        The design has been described above. Now we will discuss how we actually went about implementing each of these sections. Additionally we include a description of how the software for this program was written and how each of the individual components/ROS nodes actually fit togehter. 
                    </p> 
                    <pre>
                        while (true):
                            print("test")
                    </pre>
                    <p class="lead">Infrastructure</p>
                    <p>
                    <p>
                        For our pendulum, we considered several mor complicated designs for the creation of a pendulum. We ultimately found that using a mic stand worked well enough for what we needed for this project. If we were to extend this project a good next step would be to build a more precise rig that would allow for smoother more reliable motion. Our makeshift pendulum stand is pictured below. 
                    </p>
                    <img src="img/pendulum-rig.jpg" alt="flowchart" class="img-fluid auto-mx">
                    <p>
                        Our gripper is rather primitive, but is a proof of concept for what could be used in the future if we were to continue working on this project. For the intents and purposes of our project this was sufficient for us to use to help us achieve our other goals. In the future it would be nice to have a more acutated gripper which would allow for the gripper to be opened wider and closed tigher. The gripper attachement is needed as can be seen below. Without the attachment the ball can barely be grasped by the gripper and catching the ball would have been nearly impossible. Pictured below is the gripper before and after the addition of our attachment. The overall design just involves two larger plates which are attached directly to the original gripper attachments.
                    </p>
                    <p class="lead">Vision</p>
                    <p>
                        This is a description of our vision implementation. 
                    </p>
                    <p class="lead">Modeling</p>
                    <p>
                        This is a description of our model. 
                    </p>
                    <p class="lead">Planning and Execution</p>
                    <p>
                        This is a description of our planning and execution.
                    </p>
                    <p class="lead">Software Description</p>
                    <p>
                        The overall structure of the project can be described in the below flow chart. Below we describe in more detail the launch files we used, the topics we are publishing/subscribing to, and the ROS nodes which are present.
                    </p>
                    <img src="img/pendulum-bot-flow.svg" alt="flowchart" class="img-fluid auto-mx">
                    <h5></h5>
                    <h5>Topics</h5>
                    <ul>
                        <li><code>camera/rgb/image_raw</code> - This topic is created by the kinect and represents the RGB readouts from the kinect camera. This is the image that we will run computer vision on.</li>
                        <li><code>camera/rgb/camera_info</code> - This topic is also created by the kinect and represents some camera metadata including the size of the image we are looking at.</li>
                        <li><code>camera/depth/points</code> - This is the last topic created by the kinect that we are using. This represents the depth pointcloud created by the kinect which we ultimately use to calculate the third coordinate in the kinect frame.</li>
                        <li><code>kinect/ball/location</code> - This is a topic created by our code which represents that current location of the ball in coordinates with respect to the kinect frame.</li>
                        <li><code>base/ball/location</code> - This is a topic created by our code which represents that current location of the ball in coordinates with respect to the base frame of the robot.</li>
                        <li><code>ball/goal</code> - This is a topic created by our code which represents the optimal location we have found at which the catch should be performed. Before the model finishes, this topic publishes no information.</li>
                        <li><code>ball/ready</code> - This is a topic created by our code which publishes only a boolean value representing if the robot has moved to the pose specified in the <code>ball/goal</code> topic. If true that means the movement has finished and the gripper can close once the ball nears the end effector.</li>
                        <li><code>robot/xdisplay</code> -  This is a topic that we publish to so that we can see the kinect camera image on the screen of the robot.</li>
                    </ul>
                    <h5>ROS Nodes</h5>
                    <ul>
                        <li><code>kinect_util</code> - This is our own program. The high level function of this node is to parse the information output by the kinect and turn this into the information published to the <code>kinect/ball/location</code> topic. The complete process for how this program works is described above in more detail in the section on vision.</li>
                        <li><code>to_world_frame</code> - This is our own program which subscribes to <code>kinect/ball/location</code> and outputs the transformed coordinates in the base frame as the topic <code>base/ball/location</code>.</li>
                        <li><code>model</code> - This too is a program written by us which handles the parameter estimation and modelling of the pendulum. The specific details of how this program works are described above in the model section. At a high level this node subscribes to <code>base/ball/location</code> and then published the optimal catch position to the topic <code>ball/goal</code></li>
                        <li><code>move_to_goal</code> - This is our program which handles the path planning and execution of the path. This node subscribes to <code>ball/goal</code> in order to see where the end effector should move. It then publishes to <code>ball/ready</code> when the movement has been completed and the end effector is in the desired position.</li>
                        <li><code>gripper</code> - This is the node that we wrote to control the gripper. In order to do this this node subscribes to <code>base/ball/location</code> and <code>ball/ready</code> to determine when the gripper is ready to be closed and when the ball is near enough to be closed such that the catch can be made. Once both of these are satisfied, the gripper will close with the ball in its grasp.</li>
                        <li><code>robot_state_publisher</code> - This is a node which is used for the visualization fo the robot in RViz and is also used to determine the joint state of the robot. This relies on the URDF file of the robot (baxter_description /urdf/baxter.urdf).</li>
                        <li><code>joint_state_publisher</code> - This is a node very similar to <code>robot_state_publisher</code> which is used for the visualization of the robot in RViz and is also used to determine the joint state of the robot. This relies on the URDF file of the robot (baxter_description /urdf/baxter.urdf).</li>
                        <li><code>trajectory_server</code> - This is a part of the baxter interface which is used to actually perform the actuation of the robot.</li>
                        <li><code>display_to_kinect_transformer</code> - This is a node which publishes the static transform form head_camera to camera_link (this is the frame of the kinect).</li>
                    </ul>
                    <h5>Launch Files</h5>
                    <ul>
                        <li><code>baxter_imaging pendulum.launch</code> - This is the primary launch file that we use to bring up all all of the nodes listed above. This additionally causes the other two below launch files to be run.</li>
                        <li><code>freenect_launch freenect.launch</code> - This launch file will bring up the kinect and the topics related to the kinect () </li>
                        <li><code>baxter_moveit_config demo_baxter.launch</code> - </li>
                    </ul>
                    <p class="lead">Talk about the implementation here. Describe any hardware you used or built. Illustrate with pictures and diagrams.
                        What parts did you use to build your solution? describe any software you wrote in detail. Illustrate with diagrams, flow charts,
                        and/or other appropriate visuals. This includes launch files, URDFs, etc. How does your omplete system work? Describe each step.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <section id="results" class="bg-light">
        <div class="container">
            <div class="row">
                <div class="col-8 mx-auto">
                    <h2>Results</h2>
                    <p class="lead">Talk about the results here. How well did your project work? What tasks did it perform? Illustrate with pictures
                        and at least one video.
                    </p>
                </div>
            </div>
            <div class="row">
                <div class="col-8 mx-auto">
                    <div class="embed-responsive embed-responsive-16by9">
                        <video controls="controls" name="Vision Demo" src="img/vision_demo.mov"></video>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section id="conclusion">
        <div class="container">
            <div class="row">
                <div class="col-8 mx-auto">
                    <h2>Conclusion</h2>
                    <p class="lead">Discuss your results. How well did your finished solution meet your design criteria.</p>
                </div>
            </div>
        </div>
    </section>

    <section id="team" class="bg-light">
        <div class="container">
            <div class="row my-5">
                <div class="col-8 mx-auto">
                    <h2>Team</h2>
                    <div class="row">
                        <div class="col-4  mx-auto">
                            <img src="img/cam.jpg" alt="Cameron" class="img-fluid auto-mx">
                        </div>
                        <div class="col-8 mx-auto">
                            <p class="lead"> <b>Cameron Kurotori</b></p>
                            <p>
                                Cameron is an EECS student with a background in algorithms, task automation, and software engineering. He has taken/is taking CS170, CS70, CS61 series as well as the EE16 series. This past summer he had the opportunity to be a software developer intern at IBM.
                            </p>
                            <p>
                                The main contributions of Cameron were towards the construction of the pendulum rig, the vision processing code, and the code to compute the transformation from the camera frame of the kinect to the frame of the robot. He additionally worked on the overall system design and helped to write much of the code to create the subscribers, publishers, and other ROS nodes.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
            <div class="row my-5">
                <div class="col-8 mx-auto">
                    <div class="row">
                        <div class="col-4 mx-auto">
                            <img src="img/matt.png" alt="Matt" class="img-fluid auto-mx">
                        </div>
                        <div class="col-8 mx-auto">
                            <p class="lead"> <b>Matt Owen</b></p>
                            <p>
                                Matt is an EECS student with a background in algorithms, robotics and aritifical intelligence. I have taken/am taking EE126, CS170, CS188, and DS100. In high school he was involved in robotics for four years primarily from the mechanical persepctive. Additionally he has taken classes in CAD and computer integrated manufacturing.
                            </p>
                            <p>
                                The main contributions of Matt were towards the construction of the gripper attachment, the vision processing code, the code to compute the transformation from the camera frame of the kinect to the frame of the robot, and the code to close the gripper. He also provided general assistance to the team for the other parts of the project and wrote the report/created the website along with the help of the others.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
            <div class="row my-5">
                <div class="col-8 mx-auto">
                    <div class="row">
                        <div class="col-4  mx-auto">
                            <img src="img/sean.jpg" alt="Sean" class="img-fluid auto-mx">
                        </div>
                        <div class="col-8 mx-auto">
                            <p class="lead"> <b>Sean Farhat</b></p>
                            <p>
                                Sean is an EECS student with a background in algorithms, artificial intelligence, and low-level programming, having taken courses in both the EE, CS, Math, and Cognitive Science departments. He also does research in reinforcement learning and controls for microrobots. Additionally he has experience with computer vision and OpenCV.
                            </p>
                            <p>
                                The main contributions of Sean were in the dynamics modeling of the pendulum system, the parameter estimation, the code to plan/move the arm, and the code to close the gripper. He also provided general assistance to the team for the other parts of the project.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
            <div class="row my-5">
                <div class="col-8 mx-auto">
                    <div class="row">
                        <div class="col-4  mx-auto">
                            <img src="img/swapnil.jpg" alt="Swapnil" class="img-fluid auto-mx">
                        </div>
                        <div class="col-8 mx-auto">
                            <p class="lead"> <b>Swapnil Das</b></p>
                            <p>
                                Swapnil is an EECS student with a background in algorithms, artificial intelligence and controls. He has taken CS170, CS188, EE120 and is taking EECS149 currently. His past research experience delves deeply into low-level optimization, parallelization and regression algorithms. He is currently also learning low-level control of Kobuki in EECS149 and state machine generation.
                            </p>
                            <p>
                                The main contributions of Swapnil were in the dynamics modeling of the pendulum system, the parameter estimation, the code to plan/move the arm, and the code to close the gripper. He also provided general assistance to the team for the other parts of the project.
                            </p>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section id="additional">
        <div class="container">
            <div class="row">
                <div class="col-8 mx-auto">
                    <h2>Additional Materials</h2>
                    <p class="lead">Our code</p>
                    <p>
                        The full code for this system can be found <a href="https://github.com/omatthew98/pendulum-bot">here</a>.
                    </p>
                    <p class="lead">Resources</p>
                    <ul>
                        <li>
                            The computer vision code we used was a modification of the code by Adrian Rosebrock from pyimagesearch. The tutorial we followed is <a href="https://www.pyimagesearch.com/2015/09/14/ball-tracking-with-opencv/">here</a>.
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </section>



    <!-- Footer -->
    <footer class="py-5 bg-dark">
        <div class="container">
            <p class="m-0 text-center text-white">Copyright &copy; Pendulum-Bot 2018</p>
        </div>
        <!-- /.container -->
    </footer>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Custom JavaScript for this theme -->
    <script src="js/scrolling-nav.js"></script>

</body>

</html>
